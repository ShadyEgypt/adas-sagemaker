{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os, sagemaker, subprocess, boto3\n",
    "from datetime import datetime\n",
    "from sagemaker import s3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.16.3)\n",
      "Requirement already satisfied: openvino in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2024.1.0)\n",
      "Requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (21.3)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (4.25.3)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openvino) (2024.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->onnxruntime) (3.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install onnxruntime openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose a model:\n",
    "model_name = './code/best.onnx'\n",
    "\n",
    "session = onnxruntime.InferenceSession(model_name,providers=['CPUExecutionProvider'])\n",
    "# os.system(f'mv {model_name} code/.')\n",
    "\n",
    "bashCommand = \"tar -cpzf  model.tar.gz code/\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Zip the code and model into `model.tar.gz` and upload it to specific S3 bucket\n",
    "Here permission is granted to the S3 bucket by the IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: s3://adas-deployment-bucket\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bucket = 's3://adas-deployment-bucket'\n",
    "print(f'Bucket: {bucket}')\n",
    "sess = sagemaker.Session(default_bucket=bucket.split('s3://')[-1])\n",
    "\n",
    "prefix = \"yolov8-optimized/images-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: arn:aws:iam::141178960309:role/service-role/SageMaker-s3-admin\n",
      "Model Data: s3://adas-deployment-bucket/yolov8-optimized/images-endpoint/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(f'Role: {role}')\n",
    "\n",
    "model_data = s3.S3Uploader.upload(\"model.tar.gz\", bucket + \"/\" + prefix)\n",
    "print(f'Model Data: {model_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Create the SageMaker PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(entry_point='inference.py',\n",
    "                     model_data=model_data, \n",
    "                     framework_version='1.12', \n",
    "                     py_version='py38',\n",
    "                     role=role,\n",
    "                     env={'TS_MAX_RESPONSE_SIZE':'10000', 'YOLOV8_MODEL': model_name},\n",
    "                     sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Deploy the model on SageMaker Endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify MemorySizeInMB and MaxConcurrency in the serverless config object\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "  memory_size_in_mb=3072,\n",
    "  max_concurrency=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ENDPOINT_NAME' (str)\n",
      "Endpoint Name: yolov8-optimized-images-serverless-endpoint\n",
      "----!"
     ]
    }
   ],
   "source": [
    "INSTANCE_TYPE = 'ml.t2.medium'\n",
    "ENDPOINT_NAME = 'yolov8-optimized-images-serverless-endpoint' \n",
    "\n",
    "# Store the endpoint name in the history to be accessed by 2_TestEndpoint.ipynb notebook\n",
    "%store ENDPOINT_NAME\n",
    "print(f'Endpoint Name: {ENDPOINT_NAME}')\n",
    "\n",
    "# Deploys the model to a SageMaker serverless endpoint\n",
    "serverless_predictor = model.deploy(serverless_inference_config=serverless_config,\n",
    "                                    initial_instance_count=1, \n",
    "                                    instance_type=INSTANCE_TYPE,\n",
    "                                    deserializer=JSONDeserializer(),\n",
    "                                    endpoint_name=ENDPOINT_NAME\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
